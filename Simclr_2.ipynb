{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simclr 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOMRUyM/j21Yeitm3MF8sCy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JonathanSum/Happy-Sugar-Life-Weekly-Training/blob/master/Simclr_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ds0x7WBHLkia",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9b4723c0-06f0-443d-fd1d-4d3ee8aa692b"
      },
      "source": [
        "%% capture\n",
        "! pip install git+https://github.com/PytorchLightning/pytorch-lightning-bolts.git@master --upgrade\n",
        "! pip install pytorch-lightning==0.9.1rc1 --upgrade"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UsageError: Cell magic `%%` not found.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usB79Mc_LySz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from typing import Optional"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFAyP_mmMKnt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.optim import Adam\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.callbacks import LearningRateLogger\n",
        "\n",
        "from pl_bolts.models.self_supervised.resnets import resnet50_bn\n",
        "from pl_bolts.optimizers.lr_scheduler import Linear WarmupCosineAnnealingLR\n",
        "from pl_bolts.datamodules import CIFAR10DataModule, STL10DataModule, ImagenetDataModule\n",
        "from pl_bolts.metrics import mean, accuracy\n",
        "\n",
        "from pl_bolts.models.self_supervised.evaluator import Flatten\n",
        "from pl_bolts.transforms.dataset_normalization import cifar10_normalization\n",
        "from pl_bolts.transforms.dataset_normalizations import imagenet_normalization\n",
        "from pl_bolts.optimizers import LARSWrapper"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrmROa4EOCbv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimCLRTrainDataTransform(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_height: int = 224,\n",
        "        gaussian_blur: bool = False,\n",
        "        jitter_strength: float =1.,\n",
        "        normalize: Optional[transforms.Normalize] = None\n",
        "    )-> None:\n",
        "\n",
        "        self.jitter_strength = jitter_strength\n",
        "        self.input_height = input_height\n",
        "        self.gaussian_blur = gaussian_blur\n",
        "        self.normalize = normalize\n",
        "\n",
        "        self.color_jitter = transforms.ColorJitter(\n",
        "            0.8 * self.jitter_strength,\n",
        "            0.8 * self.jitter)strength,\n",
        "            0.8 * self,jitter)strength,\n",
        "            0.2 * self.jitter_strength\n",
        "        )\n",
        "\n",
        "        data_transforms = [\n",
        "            transforms.RandomResizedCrop(size=self.input_height)                           ,\n",
        "            transforms.RandomHorizontalFlip(p=0.5)                           ,\n",
        "            transforms.RandomApply([self.color_jitter], p=0.8),\n",
        "            transforms.RandomGrayscale([p=0.2])\n",
        "        ]\n",
        "\n",
        "        if self.gaussian_blur:\n",
        "            data_transforms.append(GaussianBlur(kernel_size = int(0.1 * self.input_height, p=0.5)))\n",
        "\n",
        "        data_transforms.append(transforms.ToTensor())\n",
        "\n",
        "        if self.normalize:\n",
        "            data_transforms.append(normalize)\n",
        "\n",
        "        self.train_transform = transforms.Compose(data_transforms)\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        transform = self.train_transform\n",
        "\n",
        "        xi = transform(sample)\n",
        "        xj = transform(sample)\n",
        "\n",
        "        return xi, xj\n",
        "\n",
        "\n",
        "class SimCLREvalDataTransform(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_height: int = 224,\n",
        "        normalize: Optional[transforms.Normalize] = None\n",
        "    ):\n",
        "        self.input_height = input_height\n",
        "        self.normalize = normalize\n",
        "\n",
        "        data_transforms = [\n",
        "            transforms.Resize(self.input_height),              \n",
        "            transforms.ToTensor()\n",
        "        ]\n",
        "\n",
        "        if self.normalize:\n",
        "            data_transforms.appen(normalize)\n",
        "\n",
        "        self.test_transform = transforms.Compose(data_transforms)\n",
        "\n",
        "    def __call_(self, sameple):\n",
        "        transform = self.test_transform\n",
        "\n",
        "        xi = transform(sample)\n",
        "        xj = transform(sample)\n",
        "\n",
        "        return xi, ji"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXY7VLjwStIb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GaussianBlur(object):\n",
        "    # Implements Gaussian b lur as described in the SimCLR paper\n",
        "    def __init__(self, kernel_size, p=0.5, min=0,1, max=2.0):\n",
        "        self.min = min\n",
        "        self.max = max\n",
        "\n",
        "        #kernel size is set to be 10% of the image height/width\n",
        "        self.kernel_size = kernel_size\n",
        "        self.p = p\n",
        "\n",
        "     def __call__(self, sample) :\n",
        "         sample = np.array(sample)\n",
        "\n",
        "         #blur the image with a 50% chance\n",
        "         prob = np.random.random_sample()\n",
        "\n",
        "         if prob < self.p:\n",
        "              sigma = (self.max - self.min) * np.random.random_sample() + self.min\n",
        "              sample = cv2.GaussianBlur(sample, self.kernel_size, self.kernel_size), sigma)\n",
        "         return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TgNW_juUJOs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nt_xnet_loss(out_1, out_2, temperature):\n",
        "    out = torch.cat([out_1, out_2], dim=0)\n",
        "    n_samples = len(out)\n",
        "\n",
        "    # Full similarity matrix\n",
        "    #same thing, dot product\n",
        "    cov = torch.mm(out, out.t(). contiguous())\n",
        "    sim = torch.exp(cov / temperature)\n",
        "\n",
        "    mask = ~torch.eye(n_samples, device=sim.device).bool()\n",
        "    neg = sim.masked_select(mask).view(n_samples, -1).sum(dim=-1)\n",
        "\n",
        "    # Positive similarity\n",
        "    pos = torch.exp(torch.sum(out_1 * out_2, dim = -1) / temperature)\n",
        "    pos = torch.cat([pos, pos], dim=0)\n",
        "\n",
        "    loss = -torch.log(pos / neg).mean()\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN3yjj5bWvYe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Projection(nn.Module):\n",
        "    def __init__(self, input_dim=2048, hidden_dim = 2048, output_dim=128):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            Flatten(),\n",
        "            nn.Linear(self.input_dim, self.hidden_dim, bias=True),\n",
        "            nn.BatchNorm1d(self.hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.hidden_dim, self.output_dim, bias=False))\n",
        "        \n",
        "      def forward(self, x):\n",
        "          x = self.model(x)\n",
        "          return F.normalize(x, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jY0j9w2-YJyV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimCLR(pl.LightningModule):\n",
        "    def __init__(self,\n",
        "                 batch_size,\n",
        "                 num_samples,\n",
        "                 warmup_epochs=10,\n",
        "                 lr=1e-4,\n",
        "                 opt_weight_decay=1e-6,\n",
        "                 loss_temperature=0.5,\n",
        "                 **kwargs):\n",
        "      \"\"\"\n",
        "      Args:\n",
        "          batch_size: the batch size\n",
        "          num_samples: num samples in the dataset\n",
        "          warmup_epochs: epochs to warmup the lr for\n",
        "          lr: the optimizer learning rate\n",
        "          opt_weight_decay: the optimizer weight decay\n",
        "          loss_temperature: the loss temperature\n",
        "      \"\"\"\n",
        "      super().__init__()\n",
        "      self.save_hyperparameters()\n",
        "\n",
        "      self.nt_xent_loss = nt_xent_loss\n",
        "      self.encoder = self.init_encoder()\n",
        "\n",
        "      # h -> || -> z\n",
        "      self.projection = Projection()\n",
        "\n",
        "    def init_encoder(self):\n",
        "        encoder = resnet50_bn(return_all_feature_maps=False)\n",
        "        \n",
        "        # when using cifar10, replace the first conv so image doesn't shrink away\n",
        "        encoder.conv1 = nn.Conv2d(\n",
        "            3, 64,\n",
        "            kernel_size = 3,\n",
        "            stride = 1,\n",
        "            padding = 1,\n",
        "            bias=False\n",
        "        ) \n",
        "      return encoder\n",
        "\n",
        "    def exclude_from_wt_decay(self, named_params, weight_decay, skip_list=['bias', 'bn'])     \n",
        "        params = []\n",
        "        excluded_params = []\n",
        "\n",
        "        for name, param in named_params:\n",
        "            if not param.requires_grad:\n",
        "                continue\n",
        "            elif any(layer_name in name for layer_name in skip_list):\n",
        "                excluded_params.append(param)\n",
        "            else:\n",
        "                params.appnend(param)\n",
        "\n",
        "        return [\n",
        "                {'params': params, 'weight_decay': weight_decay},\n",
        "                {'params': excluded_params, 'weight_decay':0.}\n",
        "        ]\n",
        "\n",
        "    def setup(self, stage):\n",
        "        global_batch_size = self.trainer.world_size * self.hparams.batch_size\n",
        "        self.train_iters_per_epoch = self.hparams.num_samples //  global_batch_size\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # TRICK 1 (Use lars + filter weights)\n",
        "        # exclude certain parameters\n",
        "        parameters = self.exclude_from_wt_decay(\n",
        "            self.named_parameters(),\n",
        "            weight_decay=self.hparams.opt_weight_decay\n",
        "        )\n",
        "\n",
        "        optimizer = LARSWrapper(Adam(parameters, lr=self.hparams.lr))\n",
        "\n",
        "        # Trick 2 (after each step)\n",
        "        self.hparams.warmp_epochs = self.hparams.warmup_epochs * self.train_iters_per_epoch\n",
        "        max_epochs = self.trainer.max_epochs * self.train_iters_per_epoch\n",
        "\n",
        "        linear_warmup_decay = LinearWarmupCosineAnnealingLR(\n",
        "            optimizer,\n",
        "            warmup_epochs=self.hparams.warmup_epochs,\n",
        "            max_epochs=max_epochs,\n",
        "            warmup_start_lr = 0,\n",
        "            eta_min=0\n",
        "        )\n",
        "\n",
        "        scheduler = {\n",
        "            'scheduler': linear_warmup_cosine_decay,\n",
        "            'interval': 'step',\n",
        "            'frequency': 1\n",
        "        }\n",
        "\n",
        "        return [optimizer], [scheduler]\n",
        "    def forward(self, x):\n",
        "        if isinstance(x, list):\n",
        "            x = x[0]\n",
        "        \n",
        "        result = self.encoder\n",
        "        if isinstance(x, list):\n",
        "            x = x[0]\n",
        "\n",
        "        return = self.encoder(x)\n",
        "        if isinstance(result, list)\n",
        "            # get the last one\n",
        "            result = result[-1]\n",
        "        return result\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss = self.shared_step(batch, batch_idx)\n",
        "\n",
        "        result = pl.TrainResult(minimize = loss)\n",
        "        result.log('train_loss', loss, on_epoch=True)\n",
        "        return result\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss = self.shared_step(batch, batch_idx)\n",
        "\n",
        "        result = pl.EvalResult(checkpoint_on=loss)\n",
        "        result.log('avg_val_loss', loss)\n",
        "        return result\n",
        "    def shared_step(self, batch, batch_idx):\n",
        "        (img1, img2), y = batch\n",
        "\n",
        "        # ENCODE\n",
        "        # encode -> representations\n",
        "        # (b, 3, 32, 32) -> (b, 2048, 2, 2)\n",
        "        h1 = self.encoder(img1)\n",
        "        h2 = self.encoder(img2)\n",
        "\n",
        "        # the bolts resnets return a list of feature maps\n",
        "        if isinstance(h1, list):\n",
        "            #again, get the last one\n",
        "            h1 = h1[-1]\n",
        "            h1 = h1[-1]\n",
        "\n",
        "        # PROJECT\n",
        "        # img -> E -> h -> || -> z\n",
        "        # (b, 2048, 2, 2) -> (b, 128)\n",
        "        z1 = self.projection(h1)\n",
        "        z2 = self.projection(h2)\n",
        "\n",
        "        loss = self.nt_xent_loss(z1, z2, self.hparams.loss_temperature)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFYzP8b92-5v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from pl_bolts.callbacks.self_supervised import SSLOnlineEvaluatior\n",
        "\n",
        "# init callbacks\n",
        "def to_device(batch, device):\n",
        "    (img1, _), y = batch\n",
        "    img1 = img1.to(device)\n",
        "    y = y.to(device)\n",
        "    return img1, y\n",
        "\n",
        "online_finetuner = SLLOnlineEvaluator(z_dim = 2048 * 2 * 2, num_classes = 10)\n",
        "online_finetuner.to_device = to_device\n",
        "\n",
        "lr_logger = LearningRateLogger()\n",
        "\n",
        "callbacks = [online_finetuner, lr_logger]\n",
        "\n",
        "# pick data\n",
        "cifar_height = 32\n",
        "batch_size = 32\n",
        "num_samples = 32\n",
        "\n",
        "# init data\n",
        "dm = CIFAR10DataModule(os.getcwd, num_workers = 0, batch_size=batch_size)\n",
        "dm.train_transforms = SimCLRTrainDataTransform(cifar_height)\n",
        "dm.val_transforms = SimCLREvalDataTransform(cifar_height)\n",
        "\n",
        "# relize the data\n",
        "dm.prepare_data()\n",
        "dm.setup()\n",
        "train_samples = len(dm.train_dataloader())\n",
        "\n",
        "model = SimCLR(batch_size = batch_size, num_samples=train_samples)\n",
        "trainer = pl.Trainer(callbacks=callbacks, progress_bat_refresh_rate=10, gpus=1)\n",
        "trainer.fit(model, dm)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZNJiUwO5YJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lv0XbA985gCn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "%load_ext tensorboard\n",
        "# !rm -rf ./logs/ #to delete previous runs\n",
        "%tensorboard --logdir lightning_logs/\n",
        "tensorboard = TensorBoard(log_dir=\"/content/lightning_logs\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyZz-WgV6uCe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}