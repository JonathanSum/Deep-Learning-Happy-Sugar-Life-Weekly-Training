{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "15-transformer_practice.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPjommPsS/PAR0e5MLc2eux",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JonathanSum/Happy-Sugar-Life-Weekly-Training/blob/master/15_transformer_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtM-b7eYtMwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as f\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsR3e2qMtlcn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "nn_Softargmax = nn.Softmax   #fox wrong name"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmhBM8h0t_2A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, p, d_input=None):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        #to-do: add more explanation on this part.\n",
        "        if d_input is None:\n",
        "            d_xq = d_xk = d_xv = d_model\n",
        "        else:\n",
        "            d_xq, d_xk, d_xv = d_input\n",
        "        \n",
        "        # Make sure that the embedding dimension of model is a multiple of number of heads\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.d_k = d_model // self.num_heads\n",
        "        \n",
        "        # There are tstill of dimension d_model. They will be split into number of heads\n",
        "        self.W_q = nn.Linear(d_xq, d_model, bias=False)\n",
        "        self.W_k = nn.Linear(d_xk, d_model, bias=False)\n",
        "        self.W_v = nn.Linear(d_xv, d_model, bias=False)\n",
        "\n",
        "        # Outputs of all sub-layers need to be of dimension d_model\n",
        "        self.W_h = nn.Linear(d_model, d_model)\n",
        "                             \n",
        "    def scaled_dot_product_attention(self, Q, K, V):\n",
        "      batch_size = Q.size(0)\n",
        "      k_length = K.size(-2)\n",
        "      \n",
        "      # Scaling by d_k so that the soft(arg)max doesn't saturate\n",
        "\n",
        "      # (bs, n_heads, q_length, dim_per_head)\n",
        "      # dim_per_head, I guess it is the self.d_k\n",
        "      Q = Q / np.sqrt(self.d_k)     \n",
        "\n",
        "\n",
        "      #K's size, I guess it is (bs, n_heads, dim_per_head, k_length)\n",
        "      # (bs, n_heads, q_length, k_length)\n",
        "      scores = torch.matmul(Q, K.transpose(2,3))\n",
        "      \n",
        "      A = nn_Softargmax(dim=-1)(scores)     #(bs, n_heads, q_length, k_length)\n",
        "\n",
        "      # Get the weighted average of the values\n",
        "      H = torch.matmul(A, V)                # (bs, n_heads, q_length, dim_per_head)\n",
        "\n",
        "      return H, A\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "      \"\"\"\n",
        "      Split the last dimension into (heads X depth)\n",
        "      Return after transpose to put in shape (batch_size X num_heads X seq+length X d_k)\n",
        "      \"\"\"\n",
        "      #I guess the reason why it is (batch_size, -1, self.num_heads, self.d_k).\n",
        "      #That is because it wants to seprate about number of heads in \n",
        "      #each sentence. And each sentense it has d_k for embedding.\n",
        "      return  x.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def group_heads(self, x, batch_size):\n",
        "      \"\"\"\n",
        "      Combine the heads again to get (batch_size X seq_length X num_heads X d_l)\n",
        "      \"\"\"\n",
        "      return x.transpose(1,2).contiguous().view(batch_size, -1,\n",
        "                                                self.num_heads * self.d_k\n",
        "                                                )\n",
        "    def forward(self, X_q, X_k, X_v):\n",
        "        batch_size, seq_length, dim = X_q.size()\n",
        "\n",
        "        # After transforming, split into num_heads\n",
        "        # Q: (bs, n_heads, q_length, dim_per_head)\n",
        "        # K: (bs, n_heads, k_length, dim_per_head)\n",
        "        # V: (bs, n_heads, v_length, dim_per_head)\n",
        "        Q = self.split_heads(self.W_q(X_q), batch_size)\n",
        "        K = self.split_heads(self.W_k(X_k), batch_size)\n",
        "        V = self.split_heads(self.W_v(X_v), batch_size)\n",
        "\n",
        "        # Calculate the attention weights for each of the heads\n",
        "        # to know how related they are\n",
        "        H_cat, A = self.scaled_dot_product_attention(Q, K, V)\n",
        "        \n",
        "        #Put all the heads back together by concat\n",
        "        # (bs, q_length, dim)\n",
        "        H_cat = self.group_heads(H_cat, batch_size)\n",
        "\n",
        "        # Final linear layer\n",
        "        H = self.W_h(H_cat)           # (bs, q_length, dim)\n",
        "\n",
        "        return H, A   "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwv32yKCR4a-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp_mha = MultiHeadAttention(d_model=512, num_heads=8, p=0)\n",
        "def print_out(Q, K, V):\n",
        "    temp_out, temp_attn = temp_mha.scaled_dot_product_attention(Q, K, V)\n",
        "    print('Attention weights are:', temp_attn.squeeze())\n",
        "    print('Output is:', temp_out.squeeze())"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxNKKK-ITpD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_K = torch.tensor(\n",
        "    [[10, 0, 0],\n",
        "     [0, 10, 0],\n",
        "     [0, 0, 10],\n",
        "     [0, 0, 10]]\n",
        ").float()[None, None]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5i1p-OGyUe-g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9081c256-0ad1-4aa7-89de-6b4b3621e8aa"
      },
      "source": [
        "test_K.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 4, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa4Wk0x-UGUe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "5e193b18-a81d-48a5-81c0-2443cb7d463e"
      },
      "source": [
        "test_V = torch.tensor(\n",
        "    [[    1,0,0],\n",
        "     [    10,0,0],\n",
        "     [    100,5,0],\n",
        "     [    1000,6,0]]\n",
        ").float()[None, None]\n",
        "test_Q = torch.tensor(\n",
        "    [[0, 10, 0]]\n",
        ").float()[None,None]\n",
        "print_out(test_Q, test_K, test_V)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention weights are: tensor([3.7266e-06, 9.9999e-01, 3.7266e-06, 3.7266e-06])\n",
            "Output is: tensor([1.0004e+01, 4.0993e-05, 0.0000e+00])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr--OcDSY9v8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "021b552c-8f8e-4c4f-8190-49c25a03e1fe"
      },
      "source": [
        "test_Q = torch.tensor(\n",
        "    [0, 0, 10]\n",
        ").float()[None,None]\n",
        "print_out(test_Q, test_K, test_V)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention weights are: tensor([1.8633e-06, 1.8633e-06, 5.0000e-01, 5.0000e-01])\n",
            "Output is: tensor([549.9979,   5.5000,   0.0000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pQnk3tmW2Dl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "3f5c19ec-9cc8-4c89-a993-7aebafb8458b"
      },
      "source": [
        "test_Q = torch.tensor(\n",
        "    [[0, 0, 10], [0, 10, 0],[10, 10, 0]]\n",
        ").float()[None,None]\n",
        "print_out(test_Q, test_K, test_V)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention weights are: tensor([[1.8633e-06, 1.8633e-06, 5.0000e-01, 5.0000e-01],\n",
            "        [3.7266e-06, 9.9999e-01, 3.7266e-06, 3.7266e-06],\n",
            "        [5.0000e-01, 5.0000e-01, 1.8633e-06, 1.8633e-06]])\n",
            "Output is: tensor([[5.5000e+02, 5.5000e+00, 0.0000e+00],\n",
            "        [1.0004e+01, 4.0993e-05, 0.0000e+00],\n",
            "        [5.5020e+00, 2.0497e-05, 0.0000e+00]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Za2yGgu1aVml",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "d05eebef-e715-4dfc-a4b6-2cb3e6e6563f"
      },
      "source": [
        "test_Q = torch.tensor(\n",
        "    [[0, 0, 10], [0, 10, 0], [10, 10, 0]]\n",
        ").float()[None,None]\n",
        "print_out(test_Q, test_K, test_V)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention weights are: tensor([[1.8633e-06, 1.8633e-06, 5.0000e-01, 5.0000e-01],\n",
            "        [3.7266e-06, 9.9999e-01, 3.7266e-06, 3.7266e-06],\n",
            "        [5.0000e-01, 5.0000e-01, 1.8633e-06, 1.8633e-06]])\n",
            "Output is: tensor([[5.5000e+02, 5.5000e+00, 0.0000e+00],\n",
            "        [1.0004e+01, 4.0993e-05, 0.0000e+00],\n",
            "        [5.5020e+00, 2.0497e-05, 0.0000e+00]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGzJArQyaSjQ",
        "colab_type": "text"
      },
      "source": [
        "We can see it will return two things.\n",
        "First is the index that it focus that is very similar from Q and K, index is given from the Attention argmax.\n",
        "In addition, the output is the average of the element in the value of that index or index(s). \n",
        "\n",
        "Average here is sum(element)/(number of index)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhtDh6sQY3AG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9dc95ce1-499b-4a45-c9e8-2b3f2e053350"
      },
      "source": [
        "test_K = torch.tensor(\n",
        "    [[10, 0, 0],\n",
        "     [ 0,0, 10],\n",
        "     [ 0, 0,10],\n",
        "     [ 0, 0,10]]\n",
        ").float()[None,None]\n",
        "\n",
        "test_V = torch.tensor(\n",
        "    [[   1,0,0],\n",
        "     [  10,0,0],\n",
        "     [ 100,5,0],\n",
        "     [1000,6,0]]\n",
        ").float()[None,None]\n",
        "\n",
        "test_Q = torch.tensor(\n",
        "    [[0, 0, 10]]\n",
        ").float()[None,None]\n",
        "print_out(test_Q, test_K, test_V)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention weights are: tensor([1.2422e-06, 3.3333e-01, 3.3333e-01, 3.3333e-01])\n",
            "Output is: tensor([369.9995,   3.6667,   0.0000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omrM9fz5dIbn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, d_model, hidden_dim, p):\n",
        "        super().__init__()\n",
        "        self.k1convL1 = nn.Linear(d_model, hidden_dim)\n",
        "        self.k1convL2 = nn.Linear(hidden_dim, d_model)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.k1convL1(x)\n",
        "      x = self.activation(x)\n",
        "      x = self.k1convL2(x)\n",
        "      return x"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKkX0uvqeE0z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, conv_hidden_dim, p = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_head, p)\n",
        "        self.cnn = CNN(d_model, conv_hidden_dim, p)\n",
        "\n",
        "        self.layernorm1 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
        "        self.layernorm2 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
        "    def forward(self, x):\n",
        "\n",
        "      # Multi-head attention\n",
        "      attn_output, _ = self.mha(x, x, x)  #(batch_size, input_seq_len, d_model)\n",
        "\n",
        "      # Layer norm after adding the residual connection\n",
        "      out1 = self.layernorm1(x + attn_output)   #(batch_size, input seq_len, d_model)\n",
        "\n",
        "      #Feed forward\n",
        "      cnn_output = self.cnn(out1)   # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "      #Second layer norm after adding residual connection\n",
        "      out2 = self.layernorm2(out1 + cnn_output)   # (batch_size, input seq_len, d_model)\n",
        "      return out2\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Obg0-EHtoFgJ",
        "colab_type": "text"
      },
      "source": [
        "##Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QV5SJ-uLoHHW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_sinusoidal_embeddings(nb_p, dim, E):\n",
        "    theta = np.array([\n",
        "        [p / np.power(10000, 2* (j//2) / dim) for j in range(dim)]                      \n",
        "        for p in range(nb_p)\n",
        "    ])\n",
        "    E[:, 0::2] = torch.FloatTensor(np.sin(theta[:, 0::2]))\n",
        "    E[:, 1::2] = torch.FloatTensor(np.sin(theta[:, 1::2]))\n",
        "    E.detach_()\n",
        "    E.requires_grad = False\n",
        "    E = E.to(device)\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab_size, max_position_embeddings, p):\n",
        "        super().__init__()\n",
        "        self.word_embeddings = nn.Embedding(vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}