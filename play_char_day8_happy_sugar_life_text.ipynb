{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "play_char.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JonathanSum/Deep-Learning-Happy-Sugar-Life-Weekly-Training/blob/master/play_char.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCFIQ7kineT4"
      },
      "source": [
        "## Train a character-level GPT on some text data\n",
        "\n",
        "The inputs here are simple text files, which we chop up to individual characters and then train GPT on. So you could say this is a char-transformer instead of a char-rnn. Doesn't quite roll off the tongue as well. In this example we will feed it some Shakespeare, which we'll get it to predict character-level."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwLnWDsbnqvI",
        "outputId": "d4aa86bb-7083-465f-c0c4-3e1b2d49ae46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "!git clone https://github.com/JonathanSum/minGPT.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'minGPT'...\n",
            "remote: Enumerating objects: 4, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 186 (delta 0), reused 0 (delta 0), pack-reused 182\u001b[K\n",
            "Receiving objects: 100% (186/186), 1.38 MiB | 2.97 MiB/s, done.\n",
            "Resolving deltas: 100% (105/105), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HflJXnwneT8"
      },
      "source": [
        "# set up logging\n",
        "import logging\n",
        "logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO,\n",
        ")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tatTHaGhneUQ"
      },
      "source": [
        "# make deterministic\n",
        "from minGPT.mingpt.utils import set_seed\n",
        "# set_seed(42)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeyh0_UGneUi"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-F1F6SBneUw"
      },
      "source": [
        "import math\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, block_size):\n",
        "        chars = sorted(list(set(data)))\n",
        "        data_size, vocab_size = len(data), len(chars)\n",
        "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
        "        \n",
        "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
        "        self.block_size = block_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.data = data\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # grab a chunk of (block_size + 1) characters from the data\n",
        "        chunk = self.data[idx:idx + self.block_size + 1]\n",
        "        # encode every character to an integer\n",
        "        dix = [self.stoi[s] for s in chunk]\n",
        "        \"\"\"\n",
        "        arrange data and targets so that the first i elements of x\n",
        "        will be asked to predict the i-th element of y. Notice that\n",
        "        the eventual language model will actually make block_size\n",
        "        individual predictions at the same time based on this data,\n",
        "        so we are being clever and amortizing the cost of the forward\n",
        "        pass of the network. So for example if block_size is 4, then\n",
        "        we could e.g. sample a chunk of text \"hello\", the integers in\n",
        "        x will correspond to \"hell\" and in y will be \"ello\". This will\n",
        "        then actually \"multitask\" 4 separate examples at the same time\n",
        "        in the language model:\n",
        "        - given just \"h\", please predict \"e\" as next\n",
        "        - given \"he\" please predict \"l\" next\n",
        "        - given \"hel\" predict \"l\" next\n",
        "        - given \"hell\" predict \"o\" next\n",
        "        \n",
        "        In addition, because the DataLoader will create batches of examples,\n",
        "        every forward/backward pass during traning will simultaneously train\n",
        "        a LOT of predictions, amortizing a lot of computation. In particular,\n",
        "        for a batched input of integers X (B, T) where B is batch size and\n",
        "        T is block_size and Y (B, T), the network will during training be\n",
        "        simultaneously training to make B*T predictions, all at once! Of course,\n",
        "        at test time we can paralellize across batch B, but unlike during training\n",
        "        we cannot parallelize across the time dimension T - we have to run\n",
        "        a forward pass of the network to recover the next single character of the \n",
        "        sequence along each batch dimension, and repeatedly always feed in a next\n",
        "        character to get the next one.\n",
        "        \n",
        "        So yes there is a big asymmetry between train/test time of autoregressive\n",
        "        models. During training we can go B*T at a time with every forward pass,\n",
        "        but during test time we can only go B at a time, T times, with T forward \n",
        "        passes.\n",
        "        \"\"\"\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
        "        return x, y\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrOYo0B9neVE"
      },
      "source": [
        "block_size = 128 # spatial extent of the model for its context"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfii1DsPn_dC",
        "outputId": "451976f5-cdc0-4487-db4b-746d6af76056",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "#minGPT.\n",
        "!git clone https://github.com/JonathanSum/MyDataSet.git"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'MyDataSet'...\n",
            "remote: Enumerating objects: 10, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 10 (delta 1), reused 7 (delta 0), pack-reused 0\n",
            "Unpacking objects: 100% (10/10), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hI1QA-RneVR",
        "outputId": "1d97e09a-2af1-469a-fe10-66bbadb65c86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# you can download this file at https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt\n",
        "text = open('MyDataSet/happy_sugar_life_text.txt', 'r').read() # don't worry we won't run out of file handles\n",
        "train_dataset = CharDataset(text, block_size) # one line of poem is roughly 50 characters"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data has 72549 characters, 72 unique.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrjU7BI0neVl",
        "outputId": "4e45ba28-3703-4fe9-ccc8-c428f2e458df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from minGPT.mingpt.model import GPT, GPTConfig\n",
        "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n",
        "                  n_layer=8, n_head=8, n_embd=512)\n",
        "model = GPT(mconf)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "09/28/2020 14:44:39 - INFO - minGPT.mingpt.model -   number of parameters: 2.535936e+07\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulJ4NKLcneV6",
        "outputId": "cb2fcbaf-eea2-49d4-e04a-6189b1946c10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from minGPT.mingpt.trainer import Trainer, TrainerConfig\n",
        "\n",
        "# initialize a trainer instance and kick off training\n",
        "tconf = TrainerConfig(max_epochs=2, batch_size=8, learning_rate=6e-4,\n",
        "                      lr_decay=True, warmup_tokens=512*20, final_tokens=2*len(train_dataset)*block_size,\n",
        "                      num_workers=4)\n",
        "trainer = Trainer(model, train_dataset, None, tconf)\n",
        "trainer.train()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 9053/9053 [07:12<00:00, 20.94it/s]\n",
            "100%|██████████| 9053/9053 [07:06<00:00, 21.23it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YO033H5WohSM",
        "outputId": "0406f54e-7c1f-424e-91ce-8dd4e2ea106b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# import gc\n",
        "# trainer = None\n",
        "# gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1005"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zD9XEDCrneWG",
        "outputId": "570408d7-81c1-48e3-87c1-e9110fe9a208",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# alright, let's sample some character-level Shakespeare\n",
        "from minGPT.mingpt.utils import sample\n",
        "\n",
        "context = \"Love\"\n",
        "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
        "y = sample(model, x, 2000, temperature=1.0, sample=True, top_k=10)[0]\n",
        "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
        "print(completion)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Love...\n",
            "Love... Love...\n",
            "Love is different for every person.\n",
            "The way it shines is different.\n",
            "But anyone can receive it.\n",
            "Once you experience it, your heart feels fulfilled in an instant.\n",
            "I'm coming!\n",
            "Welcome! I was waiting for you.\n",
            "It'll be fine!\n",
            "Love is something that can't be seen.\n",
            "Love...\n",
            "Auntie, you're hurt again.\n",
            "-Are you okay? -Yeah.\n",
            "I'm okay.\n",
            "This is also love.\n",
            "There are many different flavors of love.\n",
            "Strawberry, lemon, apple, and mint.\n",
            "There are also poisonous ones.\n",
            "Taste it. Swallow it.\n",
            "As long as there's some in the jar.\n",
            "When they run out, you add more.\n",
            "Over and over again.\n",
            "When I do that, I feel surrounded by love,\n",
            "and I feel happy.\n",
            "But, my heart still had a big hole in the middle\n",
            "all along.\n",
            "Auntie. Will you hear me out?\n",
            "What is it?\n",
            "I killed someone.\n",
            "For someone I love.\n",
            "I’ll destroy the evidence to pursue love,\n",
            "and travel far away, just the two of us. Help us out.\n",
            "Satou-chan, you're so amazing.\n",
            "Are you calling the police?\n",
            "No.\n",
            "Some of the people that came here\n",
            "also did things like that.\n",
            "But I loved them too.\n",
            "That's what my love is.\n",
            "So, of course...\n",
            "I love you too, Satou-chan!\n",
            "But I'm not your one and only love, right?\n",
            "You love everyone.\n",
            "There’s no one you love the most.\n",
            "I don't call that love.\n",
            "I'm sure of it now.\n",
            "Auntie.\n",
            "Your love is wrong.\n",
            "Ever since I was a child,\n",
            "I wanted to experience love.\n",
            "You can't see love.\n",
            "I didn't know what it looked like.\n",
            "But I finally know what it looks like now.\n",
            "When I met her,\n",
            "I felt fulfilled for the first time in my life.\n",
            "It was like a piece of snow.\n",
            "I held onto it, and tried to take care of it.\n",
            "At times, I failed.\n",
            "But still,\n",
            "she continues to fulfills me.\n",
            "It's strange.\n",
            "It keeps changing.\n",
            "I can't help but think\n",
            "it's something sweeter than sugar.\n",
            "It's priceless and special.\n",
            "That's what my love is.\n",
            "Oh. Good.\n",
            "You found love.\n",
            "I'm not going to criticize your love.\n",
            "But you're still a kid.\n",
            "A kid who can't take responsibility for herself.\n",
            "You want to overstep your bounds?\n",
            "Do it once you've grown up.\n",
            "That's right. You're completely dependen\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bE6nMFVneWS"
      },
      "source": [
        "# well that was fun"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
